{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11672b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47ca8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewer Name</th>\n",
       "      <th>Review Title</th>\n",
       "      <th>Place of Review</th>\n",
       "      <th>Up Votes</th>\n",
       "      <th>Down Votes</th>\n",
       "      <th>Month</th>\n",
       "      <th>Review text</th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kamal Suresh</td>\n",
       "      <td>Nice product</td>\n",
       "      <td>Certified Buyer, Chirakkal</td>\n",
       "      <td>889.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Feb 2021</td>\n",
       "      <td>Nice product, good quality, but price is now r...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>Don't waste your money</td>\n",
       "      <td>Certified Buyer, Hyderabad</td>\n",
       "      <td>109.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Feb 2021</td>\n",
       "      <td>They didn't supplied Yonex Mavis 350. Outside ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A. S. Raja Srinivasan</td>\n",
       "      <td>Did not meet expectations</td>\n",
       "      <td>Certified Buyer, Dharmapuri</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Apr 2021</td>\n",
       "      <td>Worst product. Damaged shuttlecocks packed in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suresh Narayanasamy</td>\n",
       "      <td>Fair</td>\n",
       "      <td>Certified Buyer, Chennai</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quite O. K. , but nowadays  the quality of the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASHIK P A</td>\n",
       "      <td>Over priced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Apr 2016</td>\n",
       "      <td>Over pricedJust â?¹620 ..from retailer.I didn'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8513</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8514</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8515</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8516</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8517</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8518 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Reviewer Name               Review Title  \\\n",
       "0               Kamal Suresh               Nice product   \n",
       "1          Flipkart Customer     Don't waste your money   \n",
       "2     A. S. Raja Srinivasan   Did not meet expectations   \n",
       "3        Suresh Narayanasamy                       Fair   \n",
       "4                  ASHIK P A                Over priced   \n",
       "...                      ...                        ...   \n",
       "8513                     NaN                        NaN   \n",
       "8514                     NaN                        NaN   \n",
       "8515                     NaN                        NaN   \n",
       "8516                     NaN                        NaN   \n",
       "8517                     NaN                        NaN   \n",
       "\n",
       "                  Place of Review  Up Votes  Down Votes     Month  \\\n",
       "0      Certified Buyer, Chirakkal     889.0        64.0  Feb 2021   \n",
       "1      Certified Buyer, Hyderabad     109.0         6.0  Feb 2021   \n",
       "2     Certified Buyer, Dharmapuri      42.0         3.0  Apr 2021   \n",
       "3        Certified Buyer, Chennai      25.0         1.0       NaN   \n",
       "4                             NaN     147.0        24.0  Apr 2016   \n",
       "...                           ...       ...         ...       ...   \n",
       "8513                          NaN       NaN         NaN       NaN   \n",
       "8514                          NaN       NaN         NaN       NaN   \n",
       "8515                          NaN       NaN         NaN       NaN   \n",
       "8516                          NaN       NaN         NaN       NaN   \n",
       "8517                          NaN       NaN         NaN       NaN   \n",
       "\n",
       "                                            Review text  Ratings  \n",
       "0     Nice product, good quality, but price is now r...        4  \n",
       "1     They didn't supplied Yonex Mavis 350. Outside ...        1  \n",
       "2     Worst product. Damaged shuttlecocks packed in ...        1  \n",
       "3     Quite O. K. , but nowadays  the quality of the...        3  \n",
       "4     Over pricedJust â?¹620 ..from retailer.I didn'...        1  \n",
       "...                                                 ...      ...  \n",
       "8513                                                NaN        5  \n",
       "8514                                                NaN        2  \n",
       "8515                                                NaN        4  \n",
       "8516                                                NaN        1  \n",
       "8517                                                NaN        4  \n",
       "\n",
       "[8518 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badmintion = pd.read_csv(\"C:/Users/Admin/Documents/PAVANTEJA FILES/reviews_badminton/data.csv\")\n",
    "badmintion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0204259b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewer_Name</th>\n",
       "      <th>Reviewer_Rating</th>\n",
       "      <th>Review_Title</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Place_of_Review</th>\n",
       "      <th>Date_of_Review</th>\n",
       "      <th>Up_Votes</th>\n",
       "      <th>Down_Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sumit  Kumar</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>I think In this price category it's best dosa ...</td>\n",
       "      <td>Certified Buyer, Lakhisarai</td>\n",
       "      <td>Sumit  Kumar</td>\n",
       "      <td>211</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BHARAT GALAGALI</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>perfect tawa for Dosa..READ MORE</td>\n",
       "      <td>Certified Buyer, Hunsur</td>\n",
       "      <td>BHARAT GALAGALI</td>\n",
       "      <td>107</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paramjeet Singh</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>Excellent tawa. Made Paneer Tikka on first day...</td>\n",
       "      <td>Certified Buyer, Rampura Phul</td>\n",
       "      <td>Paramjeet Singh</td>\n",
       "      <td>59</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Virendra  Kumar</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Nice 🙂 productREAD MORE</td>\n",
       "      <td>Certified Buyer, Chengalpattu District</td>\n",
       "      <td>Virendra  Kumar</td>\n",
       "      <td>77</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jyoti solanki</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Best in the market!</td>\n",
       "      <td>Delivery man is also good....READ MORE</td>\n",
       "      <td>Certified Buyer, Mumbai</td>\n",
       "      <td>jyoti solanki</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>Sandeep Mohapatra</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Fair</td>\n",
       "      <td>No Handel includedREAD MORE</td>\n",
       "      <td>Certified Buyer, Puri</td>\n",
       "      <td>Sandeep Mohapatra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2527</th>\n",
       "      <td>Vakul  Rana</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good quality product</td>\n",
       "      <td>niceREAD MORE</td>\n",
       "      <td>Certified Buyer, Chandigarh</td>\n",
       "      <td>Vakul  Rana</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2528</th>\n",
       "      <td>Somraj  Dhungana</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Really Nice</td>\n",
       "      <td>Everything is good in this price but handle is...</td>\n",
       "      <td>Certified Buyer, Bengaluru</td>\n",
       "      <td>Somraj  Dhungana</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2529</th>\n",
       "      <td>Veeranna N C</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>Tawa OK .but Handle not regid.poor handle.READ...</td>\n",
       "      <td>Certified Buyer, Bengaluru</td>\n",
       "      <td>Veeranna N C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Just okay</td>\n",
       "      <td>NiceREAD MORE</td>\n",
       "      <td>Certified Buyer, Dharmapuri District</td>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2531 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Reviewer_Name  Reviewer_Rating           Review_Title  \\\n",
       "0          Sumit  Kumar              5.0              Wonderful   \n",
       "1       BHARAT GALAGALI              5.0  Mind-blowing purchase   \n",
       "2       Paramjeet Singh              5.0                Awesome   \n",
       "3       Virendra  Kumar              5.0          Great product   \n",
       "4         jyoti solanki              5.0    Best in the market!   \n",
       "...                 ...              ...                    ...   \n",
       "2526  Sandeep Mohapatra              3.0                   Fair   \n",
       "2527        Vakul  Rana              4.0   Good quality product   \n",
       "2528  Somraj  Dhungana               4.0            Really Nice   \n",
       "2529       Veeranna N C              3.0                   Good   \n",
       "2530  Flipkart Customer              3.0              Just okay   \n",
       "\n",
       "                                            Review_Text  \\\n",
       "0     I think In this price category it's best dosa ...   \n",
       "1                      perfect tawa for Dosa..READ MORE   \n",
       "2     Excellent tawa. Made Paneer Tikka on first day...   \n",
       "3                               Nice 🙂 productREAD MORE   \n",
       "4                Delivery man is also good....READ MORE   \n",
       "...                                                 ...   \n",
       "2526                        No Handel includedREAD MORE   \n",
       "2527                                      niceREAD MORE   \n",
       "2528  Everything is good in this price but handle is...   \n",
       "2529  Tawa OK .but Handle not regid.poor handle.READ...   \n",
       "2530                                      NiceREAD MORE   \n",
       "\n",
       "                             Place_of_Review     Date_of_Review  Up_Votes  \\\n",
       "0                Certified Buyer, Lakhisarai       Sumit  Kumar       211   \n",
       "1                    Certified Buyer, Hunsur    BHARAT GALAGALI       107   \n",
       "2              Certified Buyer, Rampura Phul    Paramjeet Singh        59   \n",
       "3     Certified Buyer, Chengalpattu District    Virendra  Kumar        77   \n",
       "4                    Certified Buyer, Mumbai      jyoti solanki        53   \n",
       "...                                      ...                ...       ...   \n",
       "2526                   Certified Buyer, Puri  Sandeep Mohapatra         0   \n",
       "2527             Certified Buyer, Chandigarh        Vakul  Rana         0   \n",
       "2528              Certified Buyer, Bengaluru  Somraj  Dhungana          0   \n",
       "2529              Certified Buyer, Bengaluru       Veeranna N C         0   \n",
       "2530    Certified Buyer, Dharmapuri District  Flipkart Customer         2   \n",
       "\n",
       "      Down_Votes  \n",
       "0             39  \n",
       "1             17  \n",
       "2              8  \n",
       "3             12  \n",
       "4              7  \n",
       "...          ...  \n",
       "2526           0  \n",
       "2527           0  \n",
       "2528           0  \n",
       "2529           0  \n",
       "2530           1  \n",
       "\n",
       "[2531 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tawa = pd.read_csv(r\"C:/Users/Admin/Documents/PAVANTEJA FILES/reviews_tawa/data.csv\")\n",
    "tawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c4e916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>reviewer_rating</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>place_of_review</th>\n",
       "      <th>Date_of_review</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>Down_votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subhro  Banerjee</td>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Great product 🤗 with great deals 😍😍 Tata Tea G...</td>\n",
       "      <td>Certified Buyer, Budge Budge</td>\n",
       "      <td>Subhro  Banerjee</td>\n",
       "      <td>236</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shiv chandra  Jha</td>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Very nice and super qwality tea taste are grea...</td>\n",
       "      <td>Certified Buyer, Saharsa</td>\n",
       "      <td>Shiv chandra  Jha</td>\n",
       "      <td>225</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>Great test great quality great price point tim...</td>\n",
       "      <td>Certified Buyer, Sri Ganganagar</td>\n",
       "      <td>Flipkart Customer</td>\n",
       "      <td>89</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DTH Y</td>\n",
       "      <td>4</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>Nice 😊READ MORE</td>\n",
       "      <td>Certified Buyer, Phaltan</td>\n",
       "      <td>DTH Y</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bhavesh Godhani</td>\n",
       "      <td>5</td>\n",
       "      <td>Classy product</td>\n",
       "      <td>Very Good Tata tea product.READ MORE</td>\n",
       "      <td>Certified Buyer, Ahmedabad</td>\n",
       "      <td>Bhavesh Godhani</td>\n",
       "      <td>69</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9165</th>\n",
       "      <td>Omm Prakash</td>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Nice for red tea.Valeu for moneyREAD MORE</td>\n",
       "      <td>Certified Buyer, Dhamanagar</td>\n",
       "      <td>Omm Prakash</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9166</th>\n",
       "      <td>Ritu Raj</td>\n",
       "      <td>4</td>\n",
       "      <td>Good choice</td>\n",
       "      <td>niceREAD MORE</td>\n",
       "      <td>Certified Buyer, Katihar District</td>\n",
       "      <td>Ritu Raj</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9167</th>\n",
       "      <td>Arun Saini</td>\n",
       "      <td>1</td>\n",
       "      <td>Terrible product</td>\n",
       "      <td>Tata Gold  Vs Tata Tea Premium👍Tata Tea Premiu...</td>\n",
       "      <td>Certified Buyer, Haridwar</td>\n",
       "      <td>Arun Saini</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9168</th>\n",
       "      <td>Amitabh Shahi</td>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>I believe that it's the best packaged tea in t...</td>\n",
       "      <td>Certified Buyer, Darbhanga</td>\n",
       "      <td>Amitabh Shahi</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9169</th>\n",
       "      <td>Rohan  Bhowmik</td>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Very good product.READ MORE</td>\n",
       "      <td>Certified Buyer, Paschim Medinipur District</td>\n",
       "      <td>Rohan  Bhowmik</td>\n",
       "      <td>69</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9170 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          reviewer_name  reviewer_rating        review_title  \\\n",
       "0      Subhro  Banerjee                5   Worth every penny   \n",
       "1     Shiv chandra  Jha                5       Great product   \n",
       "2     Flipkart Customer                5  Highly recommended   \n",
       "3                 DTH Y                4           Very Good   \n",
       "4       Bhavesh Godhani                5      Classy product   \n",
       "...                 ...              ...                 ...   \n",
       "9165       Omm Prakash                 5      Simply awesome   \n",
       "9166           Ritu Raj                4         Good choice   \n",
       "9167         Arun Saini                1    Terrible product   \n",
       "9168      Amitabh Shahi                5           Just wow!   \n",
       "9169    Rohan  Bhowmik                 5      Simply awesome   \n",
       "\n",
       "                                            review_text  \\\n",
       "0     Great product 🤗 with great deals 😍😍 Tata Tea G...   \n",
       "1     Very nice and super qwality tea taste are grea...   \n",
       "2     Great test great quality great price point tim...   \n",
       "3                                       Nice 😊READ MORE   \n",
       "4                  Very Good Tata tea product.READ MORE   \n",
       "...                                                 ...   \n",
       "9165          Nice for red tea.Valeu for moneyREAD MORE   \n",
       "9166                                      niceREAD MORE   \n",
       "9167  Tata Gold  Vs Tata Tea Premium👍Tata Tea Premiu...   \n",
       "9168  I believe that it's the best packaged tea in t...   \n",
       "9169                        Very good product.READ MORE   \n",
       "\n",
       "                                  place_of_review     Date_of_review  \\\n",
       "0                    Certified Buyer, Budge Budge   Subhro  Banerjee   \n",
       "1                        Certified Buyer, Saharsa  Shiv chandra  Jha   \n",
       "2                 Certified Buyer, Sri Ganganagar  Flipkart Customer   \n",
       "3                        Certified Buyer, Phaltan              DTH Y   \n",
       "4                      Certified Buyer, Ahmedabad    Bhavesh Godhani   \n",
       "...                                           ...                ...   \n",
       "9165                  Certified Buyer, Dhamanagar       Omm Prakash    \n",
       "9166            Certified Buyer, Katihar District           Ritu Raj   \n",
       "9167                    Certified Buyer, Haridwar         Arun Saini   \n",
       "9168                   Certified Buyer, Darbhanga      Amitabh Shahi   \n",
       "9169  Certified Buyer, Paschim Medinipur District    Rohan  Bhowmik    \n",
       "\n",
       "      up_votes  Down_votes  \n",
       "0          236          59  \n",
       "1          225          79  \n",
       "2           89          27  \n",
       "3           30           6  \n",
       "4           69          22  \n",
       "...        ...         ...  \n",
       "9165        26           5  \n",
       "9166        19           4  \n",
       "9167        13           2  \n",
       "9168        32          10  \n",
       "9169        69          29  \n",
       "\n",
       "[9170 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tea = pd.read_csv(r\"C:\\Users\\Admin\\Documents\\PAVANTEJA FILES\\reviews_tea\\data.csv\")\n",
    "tea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcf55f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Reviewer Name', 'Review Title', 'Place of Review', 'Up Votes',\n",
      "       'Down Votes', 'Month', 'Review text', 'Ratings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(badmintion.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0ffb6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a045bd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Reviewer Name               Review Title  \\\n",
      "0            Kamal Suresh               Nice product   \n",
      "1       Flipkart Customer     Don't waste your money   \n",
      "2  A. S. Raja Srinivasan   Did not meet expectations   \n",
      "3     Suresh Narayanasamy                       Fair   \n",
      "4               ASHIK P A                Over priced   \n",
      "\n",
      "               Place of Review  Up Votes  Down Votes     Month  \\\n",
      "0   Certified Buyer, Chirakkal     889.0        64.0  Feb 2021   \n",
      "1   Certified Buyer, Hyderabad     109.0         6.0  Feb 2021   \n",
      "2  Certified Buyer, Dharmapuri      42.0         3.0  Apr 2021   \n",
      "3     Certified Buyer, Chennai      25.0         1.0       NaN   \n",
      "4                          NaN     147.0        24.0  Apr 2016   \n",
      "\n",
      "                                         Review text  Ratings Reviewer_Name  \\\n",
      "0  Nice product, good quality, but price is now r...      4.0           NaN   \n",
      "1  They didn't supplied Yonex Mavis 350. Outside ...      1.0           NaN   \n",
      "2  Worst product. Damaged shuttlecocks packed in ...      1.0           NaN   \n",
      "3  Quite O. K. , but nowadays  the quality of the...      3.0           NaN   \n",
      "4  Over pricedJust â?¹620 ..from retailer.I didn'...      1.0           NaN   \n",
      "\n",
      "   Reviewer_Rating  ... Up_Votes Down_Votes reviewer_name reviewer_rating  \\\n",
      "0              NaN  ...      NaN        NaN           NaN             NaN   \n",
      "1              NaN  ...      NaN        NaN           NaN             NaN   \n",
      "2              NaN  ...      NaN        NaN           NaN             NaN   \n",
      "3              NaN  ...      NaN        NaN           NaN             NaN   \n",
      "4              NaN  ...      NaN        NaN           NaN             NaN   \n",
      "\n",
      "   review_title  review_text place_of_review  Date_of_review up_votes  \\\n",
      "0           NaN          NaN             NaN             NaN      NaN   \n",
      "1           NaN          NaN             NaN             NaN      NaN   \n",
      "2           NaN          NaN             NaN             NaN      NaN   \n",
      "3           NaN          NaN             NaN             NaN      NaN   \n",
      "4           NaN          NaN             NaN             NaN      NaN   \n",
      "\n",
      "  Down_votes  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read each CSV file\n",
    "badminton_reviews = pd.read_csv(r\"C:/Users/Admin/Documents/PAVANTEJA FILES/reviews_badminton/data.csv\")\n",
    "tawa_reviews = pd.read_csv(r\"C:/Users/Admin/Documents/PAVANTEJA FILES/reviews_tawa/data.csv\")\n",
    "tea_reviews = pd.read_csv(r\"C:/Users/Admin/Documents/PAVANTEJA FILES/reviews_tea/data.csv\")\n",
    "\n",
    "# Combine the data from all three CSV files into a single DataFrame\n",
    "all_reviews = pd.concat([badminton_reviews, tawa_reviews, tea_reviews], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined data\n",
    "print(all_reviews.head())\n",
    "\n",
    "# Optionally, save the combined data to a new CSV file\n",
    "all_reviews.to_csv(\"C:/Users/Admin/Documents/PAVANTEJA FILES/combined_reviews.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83131c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 - Status Code: 403\n",
      "Failed to retrieve page. Stopping...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "def scrape_flipkart_reviews(product_url, output_file):\n",
    "    \"\"\"Scrapes customer reviews for a given product on Flipkart.\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Reviewer Name', 'Reviewer Rating', 'Review Title', 'Review Text', 'Place of Review', 'Date of Review', 'Up Votes', 'Down Votes']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            url = f\"{product_url}&page={page_num}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            # Debugging: Print the status code to check if the request was successful\n",
    "            print(f\"Fetching page {page_num} - Status Code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(\"Failed to retrieve page. Stopping...\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Debugging: Check if the correct content is being loaded\n",
    "            print(f\"Page Content Length: {len(response.content)}\")\n",
    "\n",
    "            reviews_section = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "            if not reviews_section:\n",
    "                print(\"No reviews section found. Stopping...\")\n",
    "                break  # No more reviews found or reviews section not loading\n",
    "\n",
    "            for review in reviews_section:\n",
    "                try:\n",
    "                    reviewer_name = review.find('p', class_='_2sc7ZR').text.strip()\n",
    "                except:\n",
    "                    reviewer_name = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    reviewer_rating = review.find('div', class_='_3LWZlK').text.strip()\n",
    "                except:\n",
    "                    reviewer_rating = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    review_title = review.find('p', class_='_2-N8zT').text.strip()\n",
    "                except:\n",
    "                    review_title = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    review_text = review.find('div', class_='t-ZTKy').div.div.text.strip()\n",
    "                except:\n",
    "                    review_text = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    place_of_review = review.find('p', class_='_2mcZGG').text.split(',')[1].strip()\n",
    "                except:\n",
    "                    place_of_review = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    date_of_review = review.find('p', class_='_2sc7ZR').next_sibling.text.strip()\n",
    "                except:\n",
    "                    date_of_review = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    upvotes = review.find_all('span', {'class': '_3c3Px5'})[0].text.strip()\n",
    "                except:\n",
    "                    upvotes = '0'\n",
    "\n",
    "                try:\n",
    "                    downvotes = review.find_all('span', {'class': '_3c3Px5'})[1].text.strip()\n",
    "                except:\n",
    "                    downvotes = '0'\n",
    "\n",
    "                # Debugging: Check if each review is being parsed correctly\n",
    "                print(f\"Reviewer Name: {reviewer_name}\")\n",
    "                print(f\"Reviewer Rating: {reviewer_rating}\")\n",
    "                print(f\"Review Title: {review_title}\")\n",
    "                print(f\"Review Text: {review_text}\")\n",
    "                print(f\"Place of Review: {place_of_review}\")\n",
    "                print(f\"Date of Review: {date_of_review}\")\n",
    "                print(f\"Up Votes: {upvotes}\")\n",
    "                print(f\"Down Votes: {downvotes}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "                # Write data to CSV file\n",
    "                writer.writerow({\n",
    "                    'Reviewer Name': reviewer_name,\n",
    "                    'Reviewer Rating': reviewer_rating,\n",
    "                    'Review Title': review_title,\n",
    "                    'Review Text': review_text,\n",
    "                    'Place of Review': place_of_review,\n",
    "                    'Date of Review': date_of_review,\n",
    "                    'Up Votes': upvotes,\n",
    "                    'Down Votes': downvotes\n",
    "                })\n",
    "\n",
    "            page_num += 1\n",
    "            time.sleep(random.uniform(1, 3))  # Add a random delay to avoid overwhelming the server\n",
    "\n",
    "# Example usage:\n",
    "product_url = 'https://www.flipkart.com/yonex-mavis-350-nylon-shuttle-yellow/p/itmfcjdyhnghfyey?pid=STLEFJ7UFQGRUUR3'\n",
    "output_file = 'flipkart_reviews.csv'\n",
    "scrape_flipkart_reviews(product_url, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "201734f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "\n",
    "def scrape_flipkart_reviews(product_url, output_file):\n",
    "    \"\"\"Scrapes customer reviews for a given product on Flipkart.\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Reviewer Name', 'Reviewer Rating', 'Review Title', 'Review Text', 'Place of Review', 'Date of Review', 'Up Votes', 'Down Votes']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            url = f\"{product_url}&page={page_num}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            # Debugging: Print the status code to check if the request was successful\n",
    "            print(f\"Fetching page {page_num} - Status Code: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(\"Failed to retrieve page. Stopping...\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Debugging: Check if the correct content is being loaded\n",
    "            print(f\"Page Content Length: {len(response.content)}\")\n",
    "\n",
    "            reviews_section = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "            if not reviews_section:\n",
    "                print(\"No reviews section found. Stopping...\")\n",
    "                break  # No more reviews found or reviews section not loading\n",
    "\n",
    "            for review in reviews_section:\n",
    "                try:\n",
    "                    reviewer_name = review.find('p', class_='_2sc7ZR').text.strip()\n",
    "                except:\n",
    "                    reviewer_name = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    reviewer_rating = review.find('div', class_='_3LWZlK').text.strip()\n",
    "                except:\n",
    "                    reviewer_rating = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    review_title = review.find('p', class_='_2-N8zT').text.strip()\n",
    "                except:\n",
    "                    review_title = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    review_text = review.find('div', class_='t-ZTKy').div.div.text.strip()\n",
    "                except:\n",
    "                    review_text = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    place_of_review = review.find('p', class_='_2mcZGG').text.split(',')[1].strip()\n",
    "                except:\n",
    "                    place_of_review = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    date_of_review = review.find('p', class_='_2sc7ZR').next_sibling.text.strip()\n",
    "                except:\n",
    "                    date_of_review = 'N/A'\n",
    "\n",
    "                try:\n",
    "                    upvotes = review.find_all('span', {'class': '_3c3Px5'})[0].text.strip()\n",
    "                except:\n",
    "                    upvotes = '0'\n",
    "\n",
    "                try:\n",
    "                    downvotes = review.find_all('span', {'class': '_3c3Px5'})[1].text.strip()\n",
    "                except:\n",
    "                    downvotes = '0'\n",
    "\n",
    "                # Debugging: Check if each review is being parsed correctly\n",
    "                print(f\"Reviewer Name: {reviewer_name}\")\n",
    "                print(f\"Reviewer Rating: {reviewer_rating}\")\n",
    "                print(f\"Review Title: {review_title}\")\n",
    "                print(f\"Review Text: {review_text}\")\n",
    "                print(f\"Place of Review: {place_of_review}\")\n",
    "                print(f\"Date of Review: {date_of_review}\")\n",
    "                print(f\"Up Votes: {upvotes}\")\n",
    "                print(f\"Down Votes: {downvotes}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "                # Write data to CSV file\n",
    "                writer.writerow({\n",
    "                    'Reviewer Name': reviewer_name,\n",
    "                    'Reviewer Rating': reviewer_rating,\n",
    "                    'Review Title': review_title,\n",
    "                    'Review Text': review_text,\n",
    "                    'Place of Review': place_of_review,\n",
    "                    'Date of Review': date_of_review,\n",
    "                    'Up Votes': upvotes,\n",
    "                    'Down Votes': downvotes\n",
    "                })\n",
    "\n",
    "            page_num += 1\n",
    "            time.sleep(random.uniform(1, 3))  # Add a random delay to avoid overwhelming the server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a91b35a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1 - Status Code: 403\n",
      "Failed to retrieve page. Stopping...\n"
     ]
    }
   ],
   "source": [
    "product_url = 'https://www.flipkart.com/yonex-mavis-350-nylon-shuttle-yellow/p/itmfcjdyhnghfyey?pid=STLEFJ7UFQGRUUR3'\n",
    "output_file = 'flipkart_reviews.csv'\n",
    "scrape_flipkart_reviews(product_url, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d02135a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbf37ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch page 1, status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def scrape_flipkart_reviews(product_url, output_file):\n",
    "    # Define headers\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Open CSV file for writing\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Reviewer Name', 'Reviewer Rating', 'Review Title', 'Review Text', 'Place of Review', 'Date of Review', 'Up Votes', 'Down Votes']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            # Construct the URL for each page\n",
    "            url = f\"{product_url}&page={page_num}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch page {page_num}, status code: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Locate the review elements (adjust selectors based on the website structure)\n",
    "            reviews = soup.find_all('div', class_='_1AtVbE')\n",
    "            if not reviews:  # Break if no more reviews are found\n",
    "                print(\"No more reviews found.\")\n",
    "                break\n",
    "\n",
    "            # Extract review data\n",
    "            for review in reviews:\n",
    "                try:\n",
    "                    reviewer_name = review.find('p', class_='_2sc7ZR').get_text(strip=True)\n",
    "                    reviewer_rating = review.find('div', class_='_3LWZlK').get_text(strip=True)\n",
    "                    review_title = review.find('p', class_='_2-N8zT').get_text(strip=True)\n",
    "                    review_text = review.find('div', class_='t-ZTKy').get_text(strip=True)\n",
    "                    place_of_review = \"Unknown\"  # Adjust if location info exists\n",
    "                    date_of_review = review.find('p', class_='_2sc7ZR').find_next('p').get_text(strip=True)\n",
    "                    up_votes = review.find('span', class_='_3c3Px5').get_text(strip=True) if review.find('span', class_='_3c3Px5') else '0'\n",
    "                    down_votes = review.find_all('span', class_='_3c3Px5')[-1].get_text(strip=True) if len(review.find_all('span', class_='_3c3Px5')) > 1 else '0'\n",
    "\n",
    "                    # Write to CSV\n",
    "                    writer.writerow({\n",
    "                        'Reviewer Name': reviewer_name,\n",
    "                        'Reviewer Rating': reviewer_rating,\n",
    "                        'Review Title': review_title,\n",
    "                        'Review Text': review_text,\n",
    "                        'Place of Review': place_of_review,\n",
    "                        'Date of Review': date_of_review,\n",
    "                        'Up Votes': up_votes,\n",
    "                        'Down Votes': down_votes\n",
    "                    })\n",
    "\n",
    "                except AttributeError:\n",
    "                    # Skip reviews with missing information\n",
    "                    continue\n",
    "\n",
    "            print(f\"Scraped page {page_num}\")\n",
    "            page_num += 1\n",
    "\n",
    "# Define product URL and output file path\n",
    "product_url = 'https://www.flipkart.com/yonex-mavis-350-nylon-shuttle-yellow/p/itmfcjdyhnghfyey?pid=STLEFJ7UFQGRUUR3'\n",
    "output_file = 'flipkart_reviews.csv'\n",
    "\n",
    "# Call the scraping function\n",
    "scrape_flipkart_reviews(product_url, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a5f6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_flipkart_reviews(product_url, output_file):\n",
    "    # Ensure headers are defined properly and accessible\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    # Open the CSV file for writing\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['Reviewer Name', 'Reviewer Rating', 'Review Title', 'Review Text', 'Place of Review', 'Date of Review', 'Up Votes', 'Down Votes']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            # Construct the URL for the current page\n",
    "            url = f\"{product_url}?page={page_num}\"\n",
    "            print(f\"Scraping page: {page_num}\")\n",
    "\n",
    "            try:\n",
    "                # Send the GET request with headers\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching page {page_num}: {e}\")\n",
    "                break\n",
    "\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all reviews on the page (replace class name with actual class from Flipkart's site)\n",
    "            reviews = soup.find_all('div', class_='_27M-vq')  \n",
    "            if not reviews:\n",
    "                print(\"No more reviews found.\")\n",
    "                break\n",
    "\n",
    "            # Process each review\n",
    "            for review in reviews:\n",
    "                try:\n",
    "                    reviewer_name = review.find('p', class_='_2sc7ZR').get_text(strip=True)\n",
    "                    reviewer_rating = review.find('div', class_='_3LWZlK').get_text(strip=True)\n",
    "                    review_title = review.find('p', class_='_2-N8zT').get_text(strip=True)\n",
    "                    review_text = review.find('div', class_='t-ZTKy').get_text(strip=True)\n",
    "                    place_of_review = 'N/A'  # Placeholder as Flipkart reviews may not have this\n",
    "                    date_of_review = review.find('p', class_='_2sc7ZR').next_sibling.get_text(strip=True)\n",
    "                    up_votes = review.find('span', class_='_3c3Px5').get_text(strip=True)\n",
    "                    down_votes = review.find_all('span', class_='_3c3Px5')[1].get_text(strip=True)\n",
    "                except AttributeError:\n",
    "                    # Skip review if any field is missing\n",
    "                    continue\n",
    "\n",
    "                # Write the review data to the CSV file\n",
    "                writer.writerow({\n",
    "                    'Reviewer Name': reviewer_name,\n",
    "                    'Reviewer Rating': reviewer_rating,\n",
    "                    'Review Title': review_title,\n",
    "                    'Review Text': review_text,\n",
    "                    'Place of Review': place_of_review,\n",
    "                    'Date of Review': date_of_review,\n",
    "                    'Up Votes': up_votes,\n",
    "                    'Down Votes': down_votes,\n",
    "                })\n",
    "\n",
    "            print(f\"Finished scraping page {page_num}\")\n",
    "            page_num += 1  # Move to the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8759f25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893079e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185b061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ac166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c097c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820e2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5042449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_flipkart_reviews(product_url, output_file):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69b0b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Reviewer Name', 'Reviewer Rating', 'Review Title', 'Review Text', 'Place of Review', 'Date of Review', 'Up Votes', 'Down Votes']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83ef102e",
   "metadata": {},
   "outputs": [],
   "source": [
    " headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'\n",
    "    }\n",
    "page_num = 1\n",
    "url = f\"{product_url}?page={page_num}\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19e10bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews found. Exiting loop.\n"
     ]
    }
   ],
   "source": [
    "while True:  # Assuming you're looping to scrape multiple pages\n",
    "    reviews_section = soup.find('div', class_='_37M3u')\n",
    "    \n",
    "    if not reviews_section:\n",
    "        print(\"No more reviews found. Exiting loop.\")\n",
    "        break  # Exiting the loop when no reviews are found\n",
    "\n",
    "    # Process reviews here\n",
    "    # Example: Extract and print review content\n",
    "    reviews = reviews_section.find_all('div', class_='review-class')  # Replace 'review-class' with the actual class\n",
    "    for review in reviews:\n",
    "        print(review.text)\n",
    "\n",
    "    # Add logic to go to the next page or break if on the last page\n",
    "    # Example:\n",
    "    next_page = soup.find('a', class_='next-page-class')  # Replace with the actual class\n",
    "    if not next_page:\n",
    "        print(\"No more pages to scrape. Exiting loop.\")\n",
    "        break\n",
    "    else:\n",
    "        next_url = next_page['href']\n",
    "        # Fetch the next page and update `soup` accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f802d3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews found. Exiting loop.\n"
     ]
    }
   ],
   "source": [
    "while True:  # Assuming you're looping to scrape multiple pages\n",
    "    reviews_section = soup.find('div', class_='_37M3u')\n",
    "    if not reviews_section:\n",
    "        print(\"No more reviews found. Exiting loop.\")\n",
    "        break  # Exiting the loop when no reviews are found\n",
    "\n",
    "    # Process reviews here (replace this comment with your actual processing code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4fca8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews found. Exiting loop.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    reviews_section = soup.find('div', class_='_37M3u')  # Indented\n",
    "    if not reviews_section:  # Indented\n",
    "        print(\"No more reviews found. Exiting loop.\")  # Indented\n",
    "        break  # Indented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d9312da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews found. Exiting loop.\n"
     ]
    }
   ],
   "source": [
    "while True:  # Infinite loop until no more reviews are found\n",
    "    reviews_section = soup.find('div', class_='_37M3u')\n",
    "    if not reviews_section:\n",
    "        print(\"No more reviews found. Exiting loop.\")\n",
    "        break\n",
    "\n",
    "    # Process reviews here\n",
    "    print(\"Processing reviews...\")\n",
    "\n",
    "    # Move to the next page (example; replace this with actual logic)\n",
    "    # Example: next_page_link = soup.find('a', class_='next-page')\n",
    "    break  # Replace with proper page navigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27af52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e878d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba6c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b1c58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f301364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews found. Exiting loop.\n"
     ]
    }
   ],
   "source": [
    "while True:  # Assuming you're looping to scrape multiple pages\n",
    "    reviews_section = soup.find('div', class_='_37M3u')\n",
    "    if not reviews_section:\n",
    "        print(\"No more reviews found. Exiting loop.\")\n",
    "        break  # Exiting the loop when no reviews are found\n",
    "\n",
    "    reviews = reviews_section.find_all('div', class_='_386Ax')\n",
    "\n",
    "    for review in reviews:\n",
    "        try:\n",
    "            reviewer_name = review.find('p', class_='_2sc7Y').text\n",
    "        except AttributeError:\n",
    "            reviewer_name = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            reviewer_rating = review.find('div', class_='_3LWZl').text\n",
    "        except AttributeError:\n",
    "            reviewer_rating = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            review_title = review.find('p', class_='_3875T').text\n",
    "        except AttributeError:\n",
    "            review_title = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            review_text = review.find('div', class_='_396bG').text\n",
    "        except AttributeError:\n",
    "            review_text = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            place_of_review = review.find('span', class_='_35-T2').text.strip()\n",
    "        except AttributeError:\n",
    "            place_of_review = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            date_of_review = review.find('span', class_='_396bG').text.strip()\n",
    "        except AttributeError:\n",
    "            date_of_review = \"N/A\"\n",
    "\n",
    "        try:\n",
    "            up_votes = review.find('div', class_='_1UsLq').text.strip()\n",
    "        except AttributeError:\n",
    "            up_votes = \"0\"\n",
    "\n",
    "        try:\n",
    "            down_votes = review.find('div', class_='_1UsLq').text.strip()\n",
    "        except AttributeError:\n",
    "            down_votes = \"0\"\n",
    "\n",
    "        # Print or save the review data\n",
    "        print({\n",
    "            \"Reviewer Name\": reviewer_name,\n",
    "            \"Rating\": reviewer_rating,\n",
    "            \"Review Title\": review_title,\n",
    "            \"Review Text\": review_text,\n",
    "            \"Place of Review\": place_of_review,\n",
    "            \"Date of Review\": date_of_review,\n",
    "            \"Up Votes\": up_votes,\n",
    "            \"Down Votes\": down_votes,\n",
    "        })\n",
    "\n",
    "    # Add logic to move to the next page here, if necessary\n",
    "    break  # Remove this break in actual multi-page scraping logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c4a046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No more reviews found. Exiting loop.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('reviews.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    # Create a CSV writer\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'Reviewer Name', 'Reviewer Rating', 'Review Title', \n",
    "        'Review Text', 'Place of Review', 'Date of Review', \n",
    "        'Up Votes', 'Down Votes'\n",
    "    ])\n",
    "    # Write the header row\n",
    "    writer.writeheader()\n",
    "\n",
    "    # Loop to scrape multiple pages\n",
    "    while True:\n",
    "        reviews_section = soup.find('div', class_='_37M3u')  # Replace with actual reviews section class\n",
    "        if not reviews_section:\n",
    "            print(\"No more reviews found. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        reviews = reviews_section.find_all('div', class_='_386Ax')  # Replace with actual review class\n",
    "\n",
    "        for review in reviews:\n",
    "            try:\n",
    "                reviewer_name = review.find('p', class_='_2sc7Y').text\n",
    "            except AttributeError:\n",
    "                reviewer_name = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                reviewer_rating = review.find('div', class_='_3LWZl').text\n",
    "            except AttributeError:\n",
    "                reviewer_rating = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                review_title = review.find('p', class_='_3875T').text\n",
    "            except AttributeError:\n",
    "                review_title = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                review_text = review.find('div', class_='_396bG').text\n",
    "            except AttributeError:\n",
    "                review_text = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                place_of_review = review.find('span', class_='_35-T2').text.strip()\n",
    "            except AttributeError:\n",
    "                place_of_review = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                date_of_review = review.find('span', class_='_396bG').text.strip()\n",
    "            except AttributeError:\n",
    "                date_of_review = \"N/A\"\n",
    "\n",
    "            try:\n",
    "                up_votes = review.find('div', class_='_1UsLq').text.strip()\n",
    "            except AttributeError:\n",
    "                up_votes = \"0\"\n",
    "\n",
    "            try:\n",
    "                down_votes = review.find('div', class_='_1UsLq').text.strip()\n",
    "            except AttributeError:\n",
    "                down_votes = \"0\"\n",
    "\n",
    "            # Write data to the CSV file\n",
    "            writer.writerow({\n",
    "                'Reviewer Name': reviewer_name,\n",
    "                'Reviewer Rating': reviewer_rating,\n",
    "                'Review Title': review_title,\n",
    "                'Review Text': review_text,\n",
    "                'Place of Review': place_of_review,\n",
    "                'Date of Review': date_of_review,\n",
    "                'Up Votes': up_votes,\n",
    "                'Down Votes': down_votes\n",
    "            })\n",
    "\n",
    "        # Add logic to go to the next page\n",
    "        next_page = soup.find('a', class_='next-page-class')  # Replace with actual class for the next page link\n",
    "        if next_page:\n",
    "            next_url = next_page['href']\n",
    "            response = requests.get(next_url)  # Fetch the next page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        else:\n",
    "            print(\"No more pages to scrape. Exiting loop.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63637f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "            page_num += 1\n",
    "            time.sleep(random.uniform(1, 3))  # Add a random delay to avoid detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce582c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3c648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
